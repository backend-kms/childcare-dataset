import streamlit as st
from ai_client import GeminiClient
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import time
import config

st.set_page_config(page_title="Childcare RAG Chatbot")
st.title("Chat with Gemini AI üçºüë∂üèª")

@st.cache_resource
def load_gemini():
    return GeminiClient(api_key=config.GOOGLE_API_KEY)

@st.cache_resource
def load_vectorstore():
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sbert-nli",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    persist_dir = "./vector_db"
    return Chroma(embedding_function=embedding_model, persist_directory=persist_dir)

gemini = load_gemini()
vectorstore = load_vectorstore()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

def run_rag(query: str, top_k: int = 50):
    docs_with_scores = vectorstore.similarity_search_with_score(query, k=top_k)
    seen_ids = set()
    context = []
    for doc, score in docs_with_scores:
        para_id = doc.metadata.get("paragraph_id") or doc.page_content[:30]
        if para_id in seen_ids:
            continue
        seen_ids.add(para_id)
        score = score if score <= 1.0 else 0.0
        context.append({
            "bookname": doc.metadata.get("bookname", "Ï†ïÎ≥¥ ÏóÜÏùå"),
            "chapter_name": doc.metadata.get("chapter_name", "Ï†ïÎ≥¥ ÏóÜÏùå"),
            "sub_chapter_name": doc.metadata.get("sub_chapter_name", "Ï†ïÎ≥¥ ÏóÜÏùå"),
            "paragraph_id": para_id,
            "content": doc.metadata.get("content") or doc.page_content,
            "score": score
        })
    answer = gemini.generate_response(query, context)
    return answer, context

for chat in st.session_state.chat_history:
    with st.chat_message(chat["role"]):
        st.markdown(chat["message"])
    if chat["role"] == "ai":
        with st.expander("ÎãµÎ≥Ä ÏÉùÏÑ±Ïóê ÏÇ¨Ïö©Îêú Ï∂úÏ≤ò Î≥¥Í∏∞"):
            for i, c in enumerate(chat.get("context", [])[:3], 1):
                st.markdown(f"#### Ï∂úÏ≤ò {i} ({c['score']*100:.1f}% Ïú†ÏÇ¨)")
                st.markdown(f"- **Ï±Ö Ïù¥Î¶Ñ:** {c['bookname']}")
                st.markdown(f"- **Ï±ïÌÑ∞:** {c['chapter_name']} - {c['sub_chapter_name']}")
                st.markdown(f"- **Î¨∏Îã® ID:** {c['paragraph_id']}")
                st.info(c['content'].replace("\n", "  \n"))

if prompt := st.chat_input("ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî:"):
    st.session_state.chat_history.append({"role": "user", "message": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("ai"):
        message_placeholder = st.empty()
        full_response = ""
        answer, context = run_rag(prompt, top_k=50)

        for char in answer:
            full_response += char
            message_placeholder.markdown(full_response + "‚ñå")
            time.sleep(0.01)
        message_placeholder.markdown(full_response)

    # AI ÎãµÎ≥ÄÍ≥º contextÎ•º Ìï®Íªò Ï†ÄÏû•
    st.session_state.chat_history.append({
        "role": "ai",
        "message": full_response,
        "context": context  # context Ìè¨Ìï®
    })

    latest_ai = next((c for c in reversed(st.session_state.chat_history) if c["role"] == "ai"), None)
    if latest_ai and latest_ai["context"]:  # contextÍ∞Ä ÏûàÏùÑ ÎïåÎßå ÌëúÏãú
        with st.expander("ÎãµÎ≥Ä ÏÉùÏÑ±Ïóê ÏÇ¨Ïö©Îêú Ï∂úÏ≤ò Î≥¥Í∏∞"):
            for i, c in enumerate(latest_ai["context"][:3], 1):
                st.markdown(f"#### Ï∂úÏ≤ò {i} ({min(c['score'],1.0)*100:.1f}% Ïú†ÏÇ¨)")
                st.markdown(f"- **Ï±Ö Ïù¥Î¶Ñ:** {c['bookname']}")
                st.markdown(f"- **Ï±ïÌÑ∞:** {c['chapter_name']} - {c['sub_chapter_name']}")
                st.markdown(f"- **Î¨∏Îã® ID:** {c['paragraph_id']}")
                text = c['content']
                if "ÎÇ¥Ïö©:" in text:
                    text = text.split("ÎÇ¥Ïö©:")[-1].strip()
                st.info(text[:300] + ("..." if len(text) > 300 else ""))