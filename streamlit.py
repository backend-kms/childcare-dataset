import streamlit as st
from ai_client import GeminiClient
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import config
import time

st.set_page_config(page_title="Childcare RAG Chatbot")
st.title("Chat with Gemini AI ğŸ¼ğŸ‘¶ğŸ»")

# GeminiClient ì´ˆê¸°í™”
@st.cache_resource
def load_gemini():
    return GeminiClient(api_key=config.GOOGLE_API_KEY)

gemini = load_gemini()

# VectorStore ì´ˆê¸°í™”
@st.cache_resource
def load_vectorstore():
    embedding_model = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sbert-nli",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    persist_dir = "./vector_db"
    vectorstore = Chroma(
        embedding_function=embedding_model,
        persist_directory=persist_dir
    )
    return vectorstore

vectorstore = load_vectorstore()

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# í˜„ì¬ ì…ë ¥ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
if "current_prompt" not in st.session_state:
    st.session_state["current_prompt"] = None

# RAG í•¨ìˆ˜
def run_rag(query: str, top_k=50):
    docs_with_scores = vectorstore.similarity_search_with_score(query, k=top_k)

    seen_ids = set()
    context = []
    for d, score in docs_with_scores:
        if score < 0.2:
            continue
        para_id = d.metadata.get("paragraph_id") or d.page_content[:30]
        if para_id in seen_ids:
            continue
        seen_ids.add(para_id)

        context.append({
            "bookname": d.metadata.get("bookname", "unknown"),
            "chapter_id": d.metadata.get("chapter_id", "unknown"),
            "chapter_name": d.metadata.get("chapter_name", "unknown"),
            "sub_chapter_name": d.metadata.get("sub_chapter_name", "unknown"),
            "paragraph_id": para_id,
            "content": d.metadata.get("content") or d.page_content,
            "raw_text": d.page_content,
            "score": min(score, 1.0)
        })

    # Gemini ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°ìš©)
    return gemini.generate_response(query, context), context

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:"):
    st.session_state["current_prompt"] = prompt

if st.session_state["current_prompt"]:
    prompt = st.session_state.pop("current_prompt")
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.session_state["chat_history"].append({"role": "user", "message": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ë©”ì‹œì§€ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë°)
    with st.chat_message("ai"):
        message_placeholder = st.empty()
        full_response = ""
        answer, context = run_rag(prompt, top_k=50)
        
        # ìŠ¤íŠ¸ë¦¬ë°ì²˜ëŸ¼ ê¸€ì ë‹¨ìœ„ ì¶œë ¥
        for char in answer:
            full_response += char
            message_placeholder.markdown(full_response)
            time.sleep(0.01)  # ì¶œë ¥ ì†ë„ ì¡°ì ˆ

        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        st.session_state["chat_history"].append({"role": "ai", "message": full_response})

# ì´ì „ ëŒ€í™” ì¶œë ¥
for chat in st.session_state["chat_history"]:
    role = chat["role"]
    message = chat["message"]
    with st.chat_message(role):
        st.markdown(message)

# ì°¸ê³  ë¬¸ì„œ í™•ì¸ (ì˜µì…˜, ìœ ì‚¬ë„ í¬í•¨, ìƒìœ„ 3ê°œ)
if st.session_state.get("chat_history") and 'context' in locals():
    with st.expander("ìƒìœ„ 3ê°œ ì¶œì²˜ ìƒì„¸ë³´ê¸°"):
        for i, c in enumerate(sorted(context, key=lambda x: x['score'], reverse=True)[:3], 1):
            st.markdown(f"### ì¶œì²˜ {i} ({c['score']*100:.1f}% ìœ ì‚¬)")
            st.markdown(f"- **ì±… ëª…:** {c['bookname']}")
            st.markdown(f"- **ì±•í„° - ì†Œì œëª©:** {c['chapter_name']} - {c['sub_chapter_name']}")
            st.markdown(f"- **ë¬¸ë‹¨ ë²ˆí˜¸:** {c['paragraph_id']}")
            st.markdown(c['content'].replace("\n", "  \n"))